# Heart-Failure

For my first project, I downloaded a dataset from the immensely popular website called Kaggle. A dataset that caught my eye was the Heart Failure Prediction dataset (https://www.kaggle.com/andrewmvd/heart-failure-clinical-data). Having always been fascinated with health and the possibility of finding insights that may improve the health of communities, the Heart Failure Prediction dataset was the perfect first dataset to play with. Who knows? Maybe I’ll find an important insight that saves millions of lives.

With this dataset, we need to create a model to assess the likelihood of death by heart failure events. I started off with the usual, importing millions of libraries for data analysis and reading the data from a CSV (Comma-Separated-Values) file. Pretty rudimentary coding. But wait for a second! Things will start to get a little interesting.
When analyzing any dataset, you need to ask yourself “What exactly is in this data?” With the following simple lines of code, you can get a sense of what your data contains. To prevent you from getting bored, I will just display the output for print(data.head()). Luckily, the data is already clean, free of NaN values, or duplicates. I just need to make sense of the data.


Looking at the values of each column, I see we have numerous Dummy Variables, variables that take either 0 or 1 to indicate the absence or presence of a variable. As you can see, the variables anemia, diabetes, high_blood_pressure, sex, smoking, and DEATH_EVENT are all dummy variables. Just from the information from the dataset and from Kaggle, I am assuming that when DEATH_EVENT = =1, that is a death event related to heart failure. We need to keep this in mind when we are analyzing the dataset using descriptive statistics or choosing a model for predictions.
Since we want to assess the likelihood of a death by a heart failure event, given the information from the dataset, I decided to create a pivot table where I can see the average values of each column for a no death event (DEATH_EVENT == 0) or a death event (DEATH_EVENT == 1). When looking at categorical variables, we need to see if the average is below or above 0.5. If the mean of a dummy variable is less than 0.5, we can say that presence of the variable is not that important to the response variable, in this case, either DEATH_EVENT == 0 or DEATH_EVENT ==1.


Interestingly enough, the means of each explanatory variable do not differ much from whether a death event occurred or not. This is puzzling with variables that are traditionally known to be correlated to heart failure, like smoking, high blood pressure, and diabetes. To visualize my dataset for further analysis, I created categorial plots, using Seaborn, for each explanatory variable.

Admittedly, this is not the best way to visualize all these graphs. I did not use either Seaborn’s FacetGrid or pairplot. Those methods are for another day and another post. Still, just from these graphs, it is clear that the death event has a small impact on the means of the explanatory variables. Only serum sodium levels, serum creatine, and ejection fractions had significant differences. I also performed a t-test and a chi-square test to further validate my findings.

From the output for my t-tests, the p-values for serum sodium, serum creatinine, and ejection reaction are all statistically significant. From my chi-square tests, it seems that age may be a significant factor in heart failure death. Okay, these p-values all correspond to the visual outputs I made earlier.
Now it is time to create my model. Per usual, I isolated my target variable, DEATH_EVENT, from my feature variables. I then split my data into training and testing sets. I set up a grid search to determine which parameters values from my parameter grid are the best in modeling the data set. Finally, I used a Random Forest Classifier to model the dataset. I ultimately decided to use the Random Forest Classifier as most of my feature variables were categorical. I could have used a Support Vector Machine as my dataset is pretty small. However, Random Forest Classifier should be more than sufficient. Finally, I used a f1 scoring as I believe both accuracy and precision are critical for this model.

As you can see from my model, I have an f1 score of 82%. Not bad for my first model. However, as I said earlier in this article, there is still much work to do. I plan on aiming to get an f1 score above 90%. To get this score, I will need to fully understand supervised learning and venture into deep learning. Luckily, COVID-19 has given me much time to explore and grow the next few months!
